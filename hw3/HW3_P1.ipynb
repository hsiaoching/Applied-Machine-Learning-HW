{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_amazon = open('data/amazon_cells_labelled.txt', 'rb')\n",
    "data_imdb = open('data/imdb_labelled.txt', 'rb')\n",
    "data_yelp = open('data/yelp_labelled.txt', 'rb')\n",
    "data = [data_amazon, data_imdb, data_yelp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "care\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "sentences = []\n",
    "labels = []\n",
    "stop_words = [str(w) for w in stopwords.words('english')]\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def preprocess(sentence):\n",
    "    result = tokenizer.tokenize(sentence.lower())\n",
    "    result = [w for w in result if w not in stop_words]\n",
    "    return result\n",
    "\n",
    "for d in data:\n",
    "    raw_text = ''.join([row for row in d])\n",
    "    sentences.extend(map(preprocess, re.split('\\t[0-1]\\n', raw_text))[:-1])\n",
    "    labels.extend(map(lambda x: int(x[1]), re.findall('\\t[0-1]\\n', raw_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Splitting dataset\n",
    "\n",
    "TRAINING_SIZE = 400\n",
    "\n",
    "train_sentences = []\n",
    "train_labels = []\n",
    "\n",
    "test_sentences = []\n",
    "test_labels = []\n",
    "\n",
    "for i in range(3):\n",
    "    n_positive = 0\n",
    "    n_negative = 0\n",
    "    for j in range(i * 1000, i * 1000 + 1000):\n",
    "        if labels[j] == 1:\n",
    "            if n_positive < (TRAINING_SIZE):\n",
    "                train_sentences.append(sentences[j])\n",
    "                train_labels.append(labels[j])\n",
    "                n_positive += 1\n",
    "            else:\n",
    "                test_sentences.append(sentences[j])\n",
    "                test_labels.append(labels[j])\n",
    "        else:\n",
    "            if n_negative < (TRAINING_SIZE):\n",
    "                train_sentences.append(sentences[j])\n",
    "                train_labels.append(labels[j])\n",
    "                n_negative += 1\n",
    "            else:\n",
    "                test_sentences.append(sentences[j])\n",
    "                test_labels.append(labels[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = {}\n",
    "n_words = 0\n",
    "for sentence in train_sentences:\n",
    "    for word in sentence:\n",
    "        if word not in corpus:\n",
    "            corpus[word] = n_words\n",
    "            n_words += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def extract_features_bog(sentences, corpus):\n",
    "    xs = np.zeros((len(sentences), len(corpus)), dtype=float)\n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(len(sentences[i])):\n",
    "            if sentences[i][j] in corpus:\n",
    "                xs[i][corpus[sentences[i][j]]] += 1\n",
    "    return xs\n",
    "    \n",
    "def postprocess(features):\n",
    "    for f in features[:5]:\n",
    "        for j in range(f.shape[0]):\n",
    "            # log-normalization\n",
    "            f[j] = np.log(f[j] + 1)\n",
    "\n",
    "train_bow_xs = extract_features_bog(train_sentences, corpus)\n",
    "test_bow_xs = extract_features_bog(test_sentences, corpus)\n",
    "\n",
    "postprocess(train_bow_xs)\n",
    "postprocess(test_bow_xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2400, 7407)\n",
      "(2400, 2400)\n",
      "(2400,)\n",
      "(2400, 7407)\n"
     ]
    }
   ],
   "source": [
    "def pca(features):\n",
    "    print features.shape\n",
    "    U, D, V = np.linalg.svd(features, full_matrices=False)\n",
    "    print U.shape\n",
    "    print D.shape\n",
    "    print V.shape\n",
    "    \n",
    "    dims = [10, 50, 100]\n",
    "    \n",
    "    reconstruced = []\n",
    "    \n",
    "    for dim in dims:\n",
    "        result = D[0] * U[:, 0] * V[0, :]\n",
    "        for i in range(1, dim):\n",
    "            result += D[i] * U[:, i] * V[i, :]\n",
    "        reconstruced.append(result)\n",
    "    return reconstruced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
